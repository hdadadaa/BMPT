# ================= Standard Library Imports =================
import os
import sys
import time
import random
import shutil
import datetime
from pathlib import Path
import argparse

# ================= Third-Party Imports ======================
import numpy as np
import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import torch.distributed as dist
import torch.nn.functional as F
from tqdm import tqdm
from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy

# ================= Project Imports ==========================
from utils.config import get_config
from utils.optimizer import build_optimizer, build_scheduler
from utils.tools import (
    AverageMeter, epoch_saving, load_checkpoint, load_checkpoint_fewshot,
    auto_resume_helper, gather_all_data, is_main_process, calculate_topk
)
from datasets.build import build_dataloader
from utils.logger import create_logger
from datasets.blending import CutmixMixupBlending
from trainers import bdc_clip
import torch._dynamo as dynamo

knowledge_path = "knowledge/"


def parse_option():
    """Parse command line arguments and config file."""
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', '-cfg', required=True, type=str, default='configs/k400/32_8.yaml')
    parser.add_argument(
        "--opts",
        help="Modify config options by adding 'KEY VALUE' pairs.",
        default=None,
        nargs='+',
    )
    parser.add_argument('--output', type=str, default="exp")
    parser.add_argument('--resume', type=str)
    parser.add_argument('--finetune_fewshot', type=str,
                        help='Path to the pretrained model for few-shot finetuning on downstream tasks.')
    parser.add_argument('--llm_json_path', type=str, help='Path to the JSON file containing prompts generated by LLMs.')
    parser.add_argument('--only_test', action='store_true')
    parser.add_argument('--batch-size', type=int)
    parser.add_argument('--accumulation-steps', type=int)
    parser.add_argument('--wise_ft', type=float, help='Wise-FT interpolation factor for model ensembling.')
    parser.add_argument("--local_rank", "--local-rank", type=int, default=-1,
                        help='Local rank for DistributedDataParallel (DDP) training.')

    args = parser.parse_args()
    config = get_config(args)
    return args, config


def main(config):
    """Main training and evaluation loop."""
    # Prepare the dataloader
    train_data, val_data, train_loader, val_loader = build_dataloader(logger, config)
    class_names = [class_name for _, class_name in train_data.classes]

    # Load negative class names from mapping file
    mapping = {}
    mapping_file = "mapping/sample_mapping_hmdb51.txt"
    if os.path.exists(mapping_file):
        with open(mapping_file, 'r', encoding='utf-8') as f:
            for line in f:
                pos, neg = line.strip().split(' -> ')
                mapping[pos] = neg
        neg_class_names = []
        for pos_sample in class_names:
            neg_sample = mapping.get(pos_sample, pos_sample)  # Fallback to itself if not found
            neg_class_names.append(neg_sample)
        logger.info(f"Loaded {len(neg_class_names)} negative class mappings")
    else:
        neg_class_names = None
        logger.info("No negative class mapping file found, skipping negative sampling")

    # Initialize the model
    model = bdc_clip.returnCLIP(config, logger=logger, class_names=class_names, neg_classnames=neg_class_names)
    model = model.cuda()

    # Define loss function and augmentation strategy from config
    mixup_fn = None
    if config.AUG.MIXUP > 0:
        criterion = SoftTargetCrossEntropy()
        mixup_fn = CutmixMixupBlending(
            num_classes=config.DATA.NUM_CLASSES,
            smoothing=config.AUG.LABEL_SMOOTH,
            mixup_alpha=config.AUG.MIXUP,
            cutmix_alpha=config.AUG.CUTMIX,
            switch_prob=config.AUG.MIXUP_SWITCH_PROB
        )
    elif config.AUG.LABEL_SMOOTH > 0:
        criterion = LabelSmoothingCrossEntropy(smoothing=config.AUG.LABEL_SMOOTH)
    else:
        criterion = nn.CrossEntropyLoss()

    # Initialize optimizer and learning rate scheduler
    optimizer = build_optimizer(config, model)
    lr_scheduler = build_scheduler(config, optimizer, len(train_loader))

    # Wrap the model with DistributedDataParallel (DDP) for multi-GPU training
    # Set find_unused_parameters=True because we removed BDC outputs but kept BDC modules
    model = torch.nn.parallel.DistributedDataParallel(
        model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False, find_unused_parameters=True
    )

    # Create a gradient scaler for mixed precision training (AMP)
    scaler = torch.cuda.amp.GradScaler()
    start_epoch, max_accuracy = 0, 0.0

    # Load checkpoint if needed
    if config.TRAIN.AUTO_RESUME:
        resume_file = auto_resume_helper(config.OUTPUT)
        if resume_file:
            config.defrost()
            config.MODEL.RESUME = resume_file
            config.freeze()
            logger.info(f'auto resuming from {resume_file}')
        else:
            logger.info(f'no checkpoint found in {config.OUTPUT}, ignoring auto resume')
    if config.MODEL.RESUME:
        start_epoch, max_accuracy = load_checkpoint(config, model, scaler, optimizer, lr_scheduler, logger)
        print("*****start epoch is {}".format(start_epoch))
        if start_epoch > 1:
            logger.info("resetting epochs no and max. accuracy to 0 after loading pre-trained weights")
            max_accuracy = 0
    if config.MODEL.FINETUNE_FEWSHOT:
        start_epoch, max_accuracy = load_checkpoint_fewshot(config, model, logger)
        print("*****start epoch is {}".format(start_epoch))
        if start_epoch > 1:
            logger.info("resetting epochs no and max. accuracy to 0 after loading pre-trained weights")
            start_epoch = 0
            max_accuracy = 0

    # Perform testing only
    if config.TEST.ONLY_TEST:
        multi_view_inference = config.TEST.MULTI_VIEW_INFERENCE
        if multi_view_inference:
            config.defrost()
            config.TEST.NUM_CLIP = 2
            config.TEST.NUM_CROP = 1
            config.freeze()
            train_data, val_data, train_loader, val_loader = build_dataloader(logger, config)
        _, _, _ = validate(val_loader, model, config)
        return

    # Use torch.compile during pre-training
    if config.TRAIN.IS_PRETRAIN:
        model = torch.compile(model, mode='default')

    # Perform training
    for epoch in range(start_epoch, config.TRAIN.EPOCHS):
        train_loader.sampler.set_epoch(epoch)
        train_one_epoch(epoch, scaler, model, criterion, optimizer, lr_scheduler, train_loader, config, mixup_fn)

        # validation and save
        if (epoch % config.SAVE_FREQ == 0) or epoch == (config.TRAIN.EPOCHS - 1):
            if not config.TRAIN.IS_PRETRAIN:
                _, top1_acc, _ = validate(val_loader, model, config)
                is_best = top1_acc > max_accuracy
                max_accuracy = max(max_accuracy, top1_acc)
                logger.info(f'Max accuracy: {max_accuracy:.2f}%')
            else:
                is_best = False
            logger.info(f'Max accuracy: {max_accuracy:.2f}%')
            if dist.get_rank() == 0 and (
                    epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1) or is_best):
                epoch_saving(config, epoch, model, scaler, max_accuracy, optimizer, lr_scheduler, logger, config.OUTPUT,
                             is_best)

    # Now doing the multi-view inference crop for videos when necessary
    # 4 CLIPs are obtained from each video, and for each CLIP, we get 3 crops (augmentations)
    multi_view_inference = config.TEST.MULTI_VIEW_INFERENCE
    if multi_view_inference:
        config.defrost()
        config.TEST.NUM_CLIP = 4
        config.TEST.NUM_CROP = 3
        config.freeze()
        train_data, val_data, train_loader, val_loader = build_dataloader(logger, config)
        acc1 = validate(val_loader, model, config)
        logger.info(f"Accuracy of the network on the {len(val_data)} test videos: {acc1:.1f}%")


def train_one_epoch(epoch, scaler, model, criterion, optimizer, lr_scheduler, train_loader, config, mixup_fn):
    """Train model for one epoch."""
    model.train()
    optimizer.zero_grad()
    num_steps = len(train_loader)

    batch_time = AverageMeter()
    loss_meter_tot = AverageMeter()
    loss_meter_cos_vl = AverageMeter()
    loss_meter_k_t = AverageMeter()  # Knowledge-text alignment loss
    top1_meter_cos_vl = AverageMeter()
    top5_meter_cos_vl = AverageMeter()
    top1_meter_k_t = AverageMeter()  # Knowledge-text alignment top1
    top5_meter_k_t = AverageMeter()  # Knowledge-text alignment top5

    start = time.time()
    end = time.time()

    for idx, batch_data in enumerate(train_loader):
        images = batch_data["imgs"].cuda(non_blocking=True)
        label_id_org = batch_data["label"].cuda(non_blocking=True)

        # Load knowledge features
        dataset_name = config.DATA.DATASET
        # filename is stored in img_metas DataContainer
        img_metas = batch_data["img_metas"].data
        filenames = [meta['filename'] for meta in img_metas]
        shortened_filenames = [filename[filename.rfind('/'):-4] for filename in filenames]
        videofile_name = [filename[1:] for filename in shortened_filenames]
        knowledge_feature_path = [
            os.path.join(knowledge_path + "/" + dataset_name + "_knowledge" + "/" + video + "_feature.npy") for
            video in videofile_name]
        knowledge_feature_list = []
        for path in knowledge_feature_path:
            feature_array = np.load(path)
            feature_tensor = torch.from_numpy(feature_array)
            feature_tensor = feature_tensor.cuda(non_blocking=True)
            knowledge_feature_list.append(feature_tensor)
        knowledge_feature = torch.cat(knowledge_feature_list, dim=0)

        label_id_org = label_id_org.reshape(-1)
        images = images.view((-1, config.DATA.NUM_FRAMES, 3) + images.size()[-2:])

        if mixup_fn is not None:
            images, label_id = mixup_fn(images, label_id_org)
        else:
            label_id = label_id_org

        with torch.cuda.amp.autocast(enabled=True):
            output_cos_vl, output_k_t, output_neg = model(images, knowledge_feature)
            loss_cos_vl = criterion(output_cos_vl, label_id)
            loss_k_t = criterion(output_k_t, label_id)  # Knowledge-text alignment loss

            # Negative sampling loss: encourage positive class logits > negative class logits
            if output_neg is not None:
                # Extract diagonal elements: logits for the correct class
                pos_logits = output_cos_vl[torch.arange(len(label_id_org)), label_id_org]
                neg_logits = output_neg[torch.arange(len(label_id_org)), label_id_org]
                # Softmax to convert to probabilities
                pos_probs = F.softmax(pos_logits.unsqueeze(-1), dim=-1).squeeze(-1)
                neg_probs = F.softmax(neg_logits.unsqueeze(-1), dim=-1).squeeze(-1)
                # Margin loss: encourage pos_probs > neg_probs + margin
                from utils.loss import compute_loss
                loss_neg = compute_loss(pos_probs, neg_probs, margin=0.1)
                total_loss = (loss_cos_vl + loss_k_t + loss_neg) / config.TRAIN.ACCUMULATION_STEPS
            else:
                total_loss = (loss_cos_vl + loss_k_t) / config.TRAIN.ACCUMULATION_STEPS

        top1_cos_vl, top5_cos_vl = calculate_topk(output_cos_vl, label_id_org)
        top1_k_t, top5_k_t = calculate_topk(output_k_t, label_id_org)

        if config.TRAIN.ACCUMULATION_STEPS == 1:
            optimizer.zero_grad()
        if config.TRAIN.OPT_LEVEL != 'O0':
            scaler.scale(total_loss).backward()
        else:
            total_loss.backward()

        if config.TRAIN.ACCUMULATION_STEPS > 1:
            if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
                lr_scheduler.step_update(epoch * num_steps + idx)
        else:
            scaler.step(optimizer)
            scaler.update()
            lr_scheduler.step_update(epoch * num_steps + idx)

        torch.cuda.synchronize()
        loss_meter_tot.update(round(total_loss.item(), 3), len(label_id))
        loss_meter_cos_vl.update(round(loss_cos_vl.item(), 3), len(label_id))
        loss_meter_k_t.update(round(loss_k_t.item(), 3), len(label_id))
        top1_meter_cos_vl.update(round(top1_cos_vl.item(), 1), 1)
        top5_meter_cos_vl.update(round(top5_cos_vl.item(), 1), 1)
        top1_meter_k_t.update(round(top1_k_t.item(), 1), 1)
        top5_meter_k_t.update(round(top5_k_t.item(), 1), 1)

        batch_time.update(time.time() - end)
        end = time.time()

        if idx % config.PRINT_FREQ == 0:
            lr = optimizer.param_groups[0]['lr']
            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)
            etas = batch_time.avg * (num_steps - idx)
            logger.info(
                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]|'
                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.4f}|'
                f'time {batch_time.val:.2f} ({batch_time.avg:.4f})|'
                f'tot_loss {loss_meter_tot.val:.2f} ({loss_meter_tot.avg:.2f})|'
                f'mem {memory_used:.0f}MB|'
                f'cos_vl: Top1={top1_meter_cos_vl.avg:.2f} Loss={loss_meter_cos_vl.avg:.2f}|'
                f'k_t: Top1={top1_meter_k_t.avg:.2f} Loss={loss_meter_k_t.avg:.2f}|')

    torch.cuda.empty_cache()
    epoch_time = time.time() - start
    logger.info(f"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}")
    logger.info(
        f"EPOCH {epoch} Loss: Total={loss_meter_tot.avg:.2f}, CLIP-VL={loss_meter_cos_vl.avg:.2f}, Knowledge={loss_meter_k_t.avg:.2f}")
    logger.info(
        f"EPOCH {epoch} Accuracy: CLIP-VL Top1={top1_meter_cos_vl.avg:.2f}/Top5={top5_meter_cos_vl.avg:.2f}, Knowledge Top1={top1_meter_k_t.avg:.2f}/Top5={top5_meter_k_t.avg:.2f}")


@torch.no_grad()
def validate(val_loader, model, config):
    """Evaluate model on validation set (Simplified: only CLIP backbone)."""
    model.eval()
    criterion = nn.CrossEntropyLoss()
    acc1_meter, acc5_meter, loss_meter = AverageMeter(), AverageMeter(), AverageMeter()
    total_pred_cos_vl, total_labels = [], []
    logger.info(f"{config.TEST.NUM_CLIP * config.TEST.NUM_CROP} views inference")
    logger.info(f"Validation set: {config.DATA.VAL_FILE}")
    logger.info(f"LLM prompt: {config.DATA.LLM_JSON}")
    loader = tqdm(val_loader, desc="Validation") if is_main_process() else val_loader
    for idx, batch_data in enumerate(loader):
        _image = batch_data["imgs"]
        label_id = batch_data["label"]
        label_id = label_id.reshape(-1)

        b, tn, c, h, w = _image.size()
        t = config.DATA.NUM_FRAMES
        n = tn // t
        _image = _image.view(b, n, t, c, h, w)

        batch_pred_cos_vl = torch.zeros((b, config.DATA.NUM_CLASSES)).cuda()

        for i in range(n):
            image = _image[:, i, :, :, :, :]
            label_id = label_id.cuda(non_blocking=True)
            image_input = image.cuda(non_blocking=True)

            if config.TRAIN.OPT_LEVEL == 'O2':
                image_input = image_input.half()
            with torch.cuda.amp.autocast():
                logits_cos_vl, _, _ = model(image_input)  # No knowledge during inference

            loss = criterion(logits_cos_vl, label_id)
            loss_meter.update(round(loss.item(), 3), b)

            pred_cos_vl = logits_cos_vl.view(b, -1).softmax(dim=-1)
            batch_pred_cos_vl += pred_cos_vl

        total_pred_cos_vl.append(batch_pred_cos_vl)
        total_labels.append(label_id)

    total_pred_cos_vl_ = torch.cat(total_pred_cos_vl, dim=0)
    total_labels_ = torch.cat(total_labels, dim=0)

    total_pred_cos_vl = gather_all_data(total_pred_cos_vl_)
    total_labels = gather_all_data(total_labels_)

    # Use only CLIP backbone predictions
    all_logits_fuse = total_pred_cos_vl

    top1_correct = (total_labels == all_logits_fuse.topk(1, dim=-1)[1].squeeze(-1)).sum().item()
    top5_correct = (total_labels.unsqueeze(-1) == all_logits_fuse.topk(5, dim=-1)[1]).sum().item()
    acc1 = top1_correct / total_labels.size(0) * 100
    acc5 = top5_correct / total_labels.size(0) * 100
    acc1_meter.update(acc1, total_labels.size(0))
    acc5_meter.update(acc5, total_labels.size(0))
    loss_meter.sync()
    print(config.DATA.VAL_FILE)
    logger.info(f"Evaluation Top-1 Accuracy: {acc1_meter.avg:.1f}, Top-5 Accuracy: {acc5_meter.avg:.1f}")
    return loss_meter.avg, acc1_meter.avg, acc5_meter.avg


if __name__ == '__main__':
    """Script entry point: parse config, initialize distributed, set up logger, and run main."""
    # Prepare config
    args, config = parse_option()

    # Init distributed
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        print(f"RANK and WORLD_SIZE in environ: {rank}/{world_size}")
    else:
        rank = -1
        world_size = -1
    torch.cuda.set_device(args.local_rank)
    torch.distributed.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)
    torch.distributed.barrier(device_ids=[args.local_rank])

    # Set random seed for reproducibility
    seed = config.SEED + dist.get_rank()
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    cudnn.benchmark = True

    # Create working directory
    Path(config.OUTPUT).mkdir(parents=True, exist_ok=True)

    # Set up logger
    global logger
    logger = create_logger(output_dir=config.OUTPUT, dist_rank=dist.get_rank(), name=f"{config.MODEL.ARCH}")
    logger.info(f"working dir: {config.OUTPUT}")

    # Save config
    if dist.get_rank() == 0:
        logger.info(config)
        shutil.copy(args.config, config.OUTPUT)

    main(config)